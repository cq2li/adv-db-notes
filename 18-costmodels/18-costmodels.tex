\documentclass[11pt]{article}

\newcommand{\lecturenumber}{22}
\newcommand{\lecturename}{Query Optimizer Cost Models}
\newcommand{\lecturedata}{2020-04-15}

\usepackage{../dbnotes}

\begin{document}

\maketitle
\thispagestyle{plain}

%% ==================================================================
%% Cost Models
%% ==================================================================
\section{Cost Models}
Cost-Based Query Planning generates an estimate of the cost of executing a particular query plan for the current state of the database. The estimates are only meaningful internally. This is independent of the search strategies.

There are three choices for Cost Model Components:
\begin{itemize}
	\item \textbf{Physical Costs}:
	Predict CPU cycles, I/O, cache misses, RAM consumption, pre-fetching, etc.
	Depends heavily on hardware.
	
	\item \textbf{Logical Costs}:
	Estimate result sizes per operator.
	Independent of the operator algorithm.
	Need estimations for operator result sizes.
	
	\item \textbf{Algorithmic Costs}:
	Complexity of the operator algorithm implementation.
\end{itemize}

%% ----------------------------------------------------
%% Disk-Based DBMS Cost Model
%% ----------------------------------------------------
\subsection*{Disk-Based DBMS Cost Model}
The number of disk accesses will always dominate the execution time of a query. CPU costs are negligible. Have to consider sequential vs. random I/O. This is easier to model if the DBMS has full control over buffer management. We will know the replacement strategy, pinning, and assume exclusive access to disk.

\begin{itemize}
	\item \textbf{\dbSys{Postgres} Cost Model}: 
	Uses a combination of CPU and I/O costs that are weighted by “magic” constant factors. Default settings are obviously for a disk-resident database without a lot of memory: Processing a tuple in memory is 400x faster than reading a tuple from disk. Sequential I/O is 4x faster than random I/O.
	
	\item \textbf{\dbSys{IBM DB2} Cost Model}: 
	Database characteristics in system catalogs. Hardware environment (microbenchmarks). Storage device characteristics (microbenchmarks). Communications bandwidth (distributed only). Memory resources (buffer pools, sort heaps). Concurrency Environment: Average number of users, Isolation level / blocking, Number of available locks.
\end{itemize}

%% ----------------------------------------------------
%% In-Memory DBMS Cost Model
%% ----------------------------------------------------
\subsection*{In-Memory DBMS Cost Model}
No I/O costs, but now we have to account for CPU and memory access costs. Memory cost is more difficult because the DBMS has no control cache management. Unknown replacement strategy, no pinning, shared caches, non-uniform memory access. The number of tuples processed per operator is a reasonable estimate for the CPU cost.

\begin{itemize}
	\item \textbf{\dbSys{Smallbase} Cost Model}:
	Two-phase model that automatically generates hardware costs from a logical model~\cite{Listgarten1996}.
	Phase \#1: Identify Execution Primitives:
	List of ops that the DBMS does when executing a query;
	Example: evaluating predicate, index probe, sorting.
	Phase \#2: Microbenchmark:
	→ On start-up, profile ops to compute CPU/memory costs;
	→ These measurements are used in formulas that compute
	operator cost based on table size.
\end{itemize}

%% ==================================================================
%% Cost Estimation
%% ==================================================================
\section{Cost Estimation}
%% ----------------------------------------------------
%% Selectivity
%% ----------------------------------------------------
\subsection*{Selectivity}
The selectivity of an operator is the percentage of data accessed for a predicate. Modeled as probability of whether a predicate on any given tuple will be satisfied. The DBMS estimates selectivities using domain constraints, precomputed statistics (Zone Maps), histograms / approximations, and sampling.

The number of tuples processed per operator depends on three factors:
\begin{itemize}
	\item The access methods available per table
	\item The distribution of values in the database’s attributes
	\item The predicates used in the query
\end{itemize} 
Simple queries are easy to estimate. More complex queries are not.

%% ----------------------------------------------------
%% Approximations
%% ----------------------------------------------------
\subsection*{Approximations}
Maintaining exact statistics about the database is expensive and slow. Use approximate data structures called sketches
to generate error-bounded estimates like Count Distinct, Quantiles, Frequent Items, and Tuple Sketch.

%% ----------------------------------------------------
%% Sampling
%% ----------------------------------------------------
\subsection*{Sampling}
Execute a predicate on a random sample of the target data set. The number of tuples to examine depends on the size of the table. There are two approaches:
\begin{itemize}
	\item \textbf{Maintain Read-Only Copy}:
	Periodically refresh to maintain accuracy.
	
	\item \textbf{Sample Real Tables}:
	Use READ UNCOMMITTED isolation.
	May read multiple versions of same logical tuple.
\end{itemize}

%% ----------------------------------------------------
%% Result Cardinality
%% ----------------------------------------------------
\subsection*{Result Cardinality}
The cardinality is the number of tuples that will be generated per operator is computed from its selectivity multiplied by the number of tuples in its input. There are three assumptions abour result cardinality:
\begin{itemize}
	\item \textbf{Uniform Data}:
	The distribution of values (except for the heavy hitters) is the same.
	
	\item \textbf{Independent Predicates}:
	The predicates on attributes are independent.
	
	\item \textbf{Inclusion Principle}:
	The domain of join keys overlap such that each key in the inner relation will also exist in the outer table.
\end{itemize}


Query opt is more important than a fast engine. Cost-based join ordering is necessary.
Cardinality estimates are routinely wrong. Try to use operators that do not rely on estimates.
Hash joins + seq scans are a robust exec model. The more indexes that are available, the more brittle the plans become (but also faster on average).
Working on accurate models is a waste of time. Better to improve cardinality estimation instead. 
% ==================================================================
% BIBLIOGRAPHY
% ==================================================================
\newpage
\bibliographystyle{abbrvnat}
\bibliography{18-costmodels}

\end{document}
