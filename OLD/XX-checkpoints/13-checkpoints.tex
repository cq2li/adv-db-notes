\documentclass[11pt]{article}

\newcommand{\lecturenumber}{13}
\newcommand{\lecturename}{Checkpoint Protocols}
\newcommand{\lecturedata}{2018-02-28}

\usepackage{../dbnotes}

\begin{document}

\maketitle
\thispagestyle{plain}

%% ==================================================================
%% IN-MEMORY CHECKPOINTS
%% ==================================================================
\section{In-Memory Checkpoints}
There are different approaches for how the DBMS can create a new checkpoint for an 
in-memory database.
The choice of approach in a DBMS is tightly coupled with its concurrency control scheme.
The checkpoint thread(s) scans each table and writes out data asynchronously to disk.

Ideal Checkpoint Properties:~\cite{p1539-ren}
\begin{itemize}
    \item Do \textbf{not} slow down regular transaction processing.
    \item Do \textbf{not} introduce unacceptable latency spikes.
    \item Do \textbf{not} require excessive memory overhead.
\end{itemize}

%% ----------------------------------------------------
%% Checkpoint Types
%% ----------------------------------------------------
\subsection{Checkpoint Types}

\textbf{Approach \#1 -- Consistent Checkpoints:}
\begin{itemize}
    \item
    Represents a consistent snapshot of the database at some point in time.
    
    \item
    No uncommitted changes.
    
    \item
    No additional processing during recovery. Still have to replay the log if not a complete 
    shutdown of the system.
\end{itemize}

\textbf{Approach \#2 -- Fuzzy Checkpoints:}
\begin{itemize}
    \item
    The snapshot could contain records updated from transactions that have not finished yet.
    
    \item
    Must do additional processing to remove those changes in the log.
    
    \item
    Write a \sql{BEGIN} and \sql{END} record to the log to know when the checkpoint was active.
\end{itemize}

%% ----------------------------------------------------
%% Checkpoint Contents
%% ----------------------------------------------------
\subsection{Checkpoint Contents}

\textbf{Approach \#1 -- Complete Checkpoint:}
\begin{itemize}
    \item
    Write out every tuple in every table regardless of whether were modified since the 
    last checkpoint.

    \item
    Nearly every system does this approach.
\end{itemize}

\textbf{Approach \#2 -- Delta Checkpoint:}
\begin{itemize}
    \item
    Write out only the tuples that were modified since the last checkpoint.
    
    \item
    Can merge checkpoints together in the background.
\end{itemize}

%% ----------------------------------------------------
%% Checkpoint Frequency
%% ----------------------------------------------------
\subsection{Checkpoint Frequency}
Taking a checkpoint too often causes the runtime performance to degrade
However, waiting a long time between checkpoints is just as bad.

\textbf{Approach \#1 -- Time-based}
\begin{itemize}
    \item
    Periodically take a new snapshot after some amount of time.
\end{itemize}

\textbf{Approach \#2 -- Log-File Size Threshold}
\begin{itemize}
    \item
    Take a new snapshot after the DBMS has written a certain amount of data to the log file.
\end{itemize}

\textbf{Approach \#3 -- On Shutdown}
\begin{itemize}
    \item Take a snapshot whenever the administrator tells the system to shutdown.
\end{itemize}

%% ==================================================================
%% CHECKPOINT ALGORITHMS
%% ==================================================================
\section{Checkpoint Algorithms}
There are several approaches for implementing a checkpoint algorithm for an in-memory 
DBMS~\cite{p265-cao}.

Checkpoint implementation primitives:
\begin{itemize}
    \item \textbf{Bulk State Copying:}
    Pause transaction execution to take a snapshot.
    
    \item \textbf{Locking/Latching:}
    Use latches to isolate the checkpoint thread from the worker threads 
    if they operate on shared regions.
    
    \item \textbf{Bulk BitMap Reset:}
    If DBMS uses BitMap to track dirty region, it must perform a bulk reset at the start of a new 
    checkpoint.
    
    \item \textbf{Memory Usage:}
    To avoid asynchronous writes, the method may need to allocate 
    additional memory for data copies
\end{itemize}

%% ----------------------------------------------------
%% Naive Snapshots
%% ----------------------------------------------------
\subsection{Naive Snapshots}
Block all transactions, and create a consistent copy of the entire database in a new 
location in memory and write the contents to disk.

Two approaches for copying
\begin{itemize}
    \item
    Do it yourself (tuple data only).
    
    \item
    Let the OS do it for your (everything).
\end{itemize}

The \dbSys{HyPer} DBMS originally used OS fork snapshots~\cite{p195-kemper}:
\begin{itemize}
    \item
    Create a snapshot of the database by forking the DBMS process.
    
    \item
    Child process contains a consistent checkpoint if there are no active transactions.
    
    \item
    Otherwise, use the in-memory undo log to roll back transactions in the child process.
    
    \item
    Continue processing transactions in the parent process.
\end{itemize}

%% ----------------------------------------------------
%% Copy-on-Update Snapshots
%% ----------------------------------------------------
\subsection{Copy-on-Update Snapshots}
During the checkpoint, transactions create new copies of data instead of overwriting it.
Copies can be at different granularities (block, tuple).
The checkpoint thread then skips anything that was created after it started.
Old data is pruned after it has been written to disk.

This is trivial to do in an multi-versioning DBMS with snapshot isolation.

Although \dbSys{VoltDB} is not a multi-versioned DBMS, it still supports consistent 
checkpoints~\cite{malviya-icde14}:
\begin{itemize}
    \item
    A special transaction starts a checkpoint and switches the DBMS into copy-on-update mode.
    
    
    \item
    Changes are no long made in-place to tables.
    The DBMS tracks whether a tuple has been inserted, deleted, or modified since the 
    checkpoint started.
    
    \item
    A separate thread scans the tables and writes tuples out to the snapshot on disk.
    It also ignores anything changed after the checkpoint and cleans up old versions.
\end{itemize}

%% ----------------------------------------------------
%% Wait-Free ZigZag
%% ----------------------------------------------------
\subsection{Wait-Free ZigZag}
The DBMS maintains two copies of the entire database. Each write only updates one copy.
This requires two BitMaps to keep track of what copy a transaction should read/write from per tuple.

Avoids the overhead of having to create copies on the fly as in the copy-on-update 
approach.

%% ----------------------------------------------------
%% Wait-Free PingPong
%% ----------------------------------------------------
\subsection{Wait-Free PingPong}
Trade extra memory + CPU to avoid pauses at the end of the checkpoint.
The DBMS maintains two copies of the entire database at all times plus a third ``base'' copy.
Pointer indicates which copy is the current master.
At the end of the checkpoint, swap these pointers.

%% ==================================================================
%% SHARED MEMORY RESTARTS
%% ==================================================================
\section{Shared Memory Restarts}
Not all DBMS restarts are due to crashes:
\begin{itemize}
    \item Updating OS libraries
    \item Hardware upgrades/fixes
    \item \textbf{Updating DBMS software}
\end{itemize}

Thus, we need a way to be able to quickly restart the DBMS without having to re-read the 
entire database from disk again.

Facebook's \dbSys{Scuba} DBMS is in-memory DBMS for time-series event analysis and anomaly 
detection. They want to be able to restart the DBMS to upgrade it to a new version without having 
to load the last checkpoint from disk back into memory~\cite{p541-goel}.

The main idea is to decouple the in-memory database lifetime from the process lifetime using 
shared memory (SHM). By storing the database shared memory, the DBMS process can restart and the 
memory contents will survive.

\textbf{Approach \#1 -- Shared Memory Heaps:}
\begin{itemize}
    \item
    All data is allocated in SHM during normal operations.
    
    \item
    Have to write a custom allocator to subdivide memory segments for thread safety and 
    scalability. Cannot use lazy allocation of backing pages with SHM.
\end{itemize}

\textbf{Approach \#2 -- Copy on Shutdown:}
\begin{itemize}
    \item
    All data is allocated in local memory during normal operations.
    
    \item
    When the admin initiates a restart command, the node halts ingesting updates and the DBMS copies 
    data from heap to SHM. Once snapshot finishes, the DBMS restarts.
    
    \item
    On start up, the DBMS checks to see whether there is a valid database in SHM to copy 
    into its heap. Otherwise, the DBMS restarts from disk.
\end{itemize}

% ==================================================================
% BIBLIOGRAPHY
% ==================================================================
\newpage
\bibliographystyle{abbrvnat}
\bibliography{13-checkpoints}

\end{document}
