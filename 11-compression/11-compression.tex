\documentclass[11pt]{article}

\newcommand{\lecturenumber}{11}
\newcommand{\lecturename}{System Catalogs and Database Compression}
\newcommand{\lecturedata}{2018-02-21}

\usepackage{../dbnotes}

\begin{document}

\maketitle
\thispagestyle{plain}

\section{System Catalogs}
    \begin{itemize}
        \item Almost every DBMS stores their database catlog in itself
        \begin{itemize}
            \item Wrap object abstraction around tuples
            \item Specialized code for "bootstrapping" catalog tables
        \end{itemize}
        \item The entire DBMS should be aware of transactions in order to automatically provide ACID guarantees for DDL commands and concurrent txns
        \item \textbf{Actions that modify the catalog should be treated like transactions to allow for ACID guarantees}
    \end{itemize}

    \subsection*{\textbf{Schema Changes}}

        \subsubsection*{\sql{add column}}
        \begin{itemize}
            \item NSM: Copy tuples into new region in memory
            \item DSM: Just create the new column segment
        \end{itemize}
        \subsubsection*{\sql{Drop column}}
        \begin{itemize}
            \item NSM \# 1: Copy tuples into new region of memory
            \item NSM \# 2: Mark column as "depracated", clean up later
            \item DSM: Drop the column and free memory
        \end{itemize}
        \subsubsection*{\sql{Change Column}}
        \begin{itemize}
            \item Check whether the conversion is allowed to happen
            \item Depends on default values
        \end{itemize}
        \subsubsection*{\sql{Create Index}}
        \begin{itemize}
            \item Scan the entire table and populate the index
            \item Have to record changes made by txns that modified the table while another txn was building the index
            \item When the scan completes, lock the table and resolve changes that were missed after the scan started
        \end{itemize}
        \subsubsection*{\sql{Drop Index}}
        \begin{itemize}
            \item Drop the index logically from the catalog
            \item It only becomes "invisible" when the txn that dropped it commits
            \item All existing txns will still have to update it
        \end{itemize}

    \subsection*{Sequences}
    \begin{itemize}
        \item Typically stored in the catalog and used for maintaining a global counter
        \item Also called "auto-increment" or "serial" keys
        \item Sequences are not maintained with the same isolation protection as regular catalog entries
        \item \textbf{Rolling back a txn that incremented a sequence does not rollback the change to that sequence}
    \end{itemize}


\section{Compression}

    \subsection*{Observations}
    \begin{itemize}
        \item I/O is the main bottle neck if the DBMS has to fetch data from disk
        \item In-Memory DBMSs are more complicated
        \item Compressing the database reduces DRAM requirements and processing
        \item Key trade-off is \textbf{speed} vs \textbf{compression ratio}
        \begin{itemize}
            \item Every In-Memory DBMSs always choose speed
        \end{itemize}
    \end{itemize}

    \subsection*{Real-World Data Characteristics}
    \begin{itemize}
        \item Data sets tend to have highly \textbf{skewed} distributions for attribute values
        \item Data sets tend to have high \textbf{correlation} between attributes of the same tuple
    \end{itemize}

    \subsection*{Goals for Database Compression}
    \begin{itemize}
        \item Must produce fixed-length values
        \item Allow the DBMS to postpone decompression as long as possible during query execution
        \item Must be a \textbf{lossless} scheme
        \begin{itemize}
            \item When a DBMS uses compression, it is always \textbf{lossless} because people don't like losing data
            \item Any kind of \textbf{lossy} compression has to be performed at the application level
            \item Some new DBMSs support approximate queries (\dbSys{BlinkDB, SnappyData, XDB, Oracle})
        \end{itemize}
    \end{itemize}

    \subsection*{Zone Maps}
    \begin{itemize}
        \item Pre-computed aggregates for blocks of data
        \item DBMS can check the zone map first to decide whether it wants to access the block
    \end{itemize}

    \subsection*{Compression Granularity}
    \begin{itemize}
        \item Block Level: Compress a block of tuples for the same table
        \item Tuple Level: Compress the contents of the entire tuple (NSM only)
        \item Attribute Level: Compress a single attribute value within one tuple. This approach can target multiple attributes for the same tuple
        \item Compress multiple values for one or more attributes stored for multiple tuples (DSM only)
    \end{itemize}

\section{Naive Compression}
\begin{itemize}
    \item Compress data using a general purpose algorithm
    \item Scope of compression is only based on the data provided as input
\end{itemize}
    \subsection*{Entropy Encoding}
    \begin{itemize}
        \item More common sequences use less bits to encode, less common sequences use more bits to encode
    \end{itemize}
    \subsection*{Dictionary Encoding}
    \begin{itemize}
        \item Build a data structure that maps data segments to an identifier. Replace those segments in the original data with a reference to the segments position in the dictionary data structure
    \end{itemize}

\begin{itemize}
    \item The data \textbf{must} be decompressed first before it can be read and (potentially) modified
    \item This limits the scope of the compression scheme
    \item These schemes also do not consider the high-level meaning or semantics of the data
\end{itemize}

\section{Encoding Schemes}

    \subsection*{Run-Length Encoding~\cite{p31-roth}}
    \begin{itemize}
        \item Compress runs of the same value in a single column into triplets
        \begin{itemize}
            \item The value of the attribute
            \item The start position in the column segment
            \item The \# of elements in the run
        \end{itemize}
        \item Requires the columns to be sorted intelligently to maximize compression opportunitites
    \end{itemize}

    \subsection*{Bitmap Encoding~\cite{franklin14}}
    \begin{itemize}
        \item Store a separate Bitmap for each unqiue value for a particular attribute where an offset in the vector corresponds to a tuple
        \item The $i^{th}$ position in the Bitmap corresponds to the $i^{th}$ tuple in the table
        \item Typically segmented into chunks to avoid allocating large blocks of contiguous memory
        \item Only practical if the value cardinality is low
    \end{itemize}

    \subsection*{Delta Encoding}
    \begin{itemize}
        \item Recording the difference between values that follow each other in the same column
        \item The base value can be stored in-line or in a separate look-up table
        \item Can be combined with RLE to get even better compression ratios
    \end{itemize}

    \subsection*{Incremental Encoding}
    \begin{itemize}
        \item Type of delta encoding whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated
        \item This works best with sorted data
    \end{itemize}

    \subsection*{Mostly Encoding}
    \begin{itemize}
        \item From \dbSys{Amazon Redshift}
        \item When the values for an attribute are "mostly" less than the largest size, you can store them as a smaller data type
        \item The remaining values that cannot be compressed are stored in their raw form in a separate space
    \end{itemize}


\section{Dictionary Compression ~\cite{p283-binnig}}
\begin{itemize}
    \item Replace frequent pattern with smaller codes
    \item Most pervasive compression scheme in DBMSs
    \item Needs to support fast encoding and decoding
    \item Needs to also support range queries
\end{itemize}

    \subsection*{Dictionary Construction Approaches}
        \subsubsection*{All at Once}
        \begin{itemize}
            \item Compute the dictionary for all the tuples at a given point of time
            \item New Tuples must use a separate dictionary or all the tuples mus tbe recomputed
        \end{itemize}
        \subsubsection*{Incremental}
        \begin{itemize}
            \item Merge new tuples in with an existing dictionary
            \item Likely requires re-encoding to existing tuples
        \end{itemize}

    \subsection*{Dictionary Scope Choices}
        \subsubsection*{Lock Level}
        \begin{itemize}
            \item Only include a subset of tuples within a single table
            \item Build dictionaries per block
            \item Potentially lower compression ratio, but adding new tuples is easier
        \end{itemize}

        \subsubsection*{Table Level}
        \begin{itemize}
            \item Construct a dictionary for the entire table
            \item Better compression ratio, but expensive to update
        \end{itemize}

        \subsubsection*{Multi-Table}
        \begin{itemize}
            \item Can be either subset or entire tables
            \item Sometimes helps with joins and set operations
        \end{itemize}

    \subsection*{Attribute Encoding}
    \begin{itemize}
        \item Instead of storing a single value per dictionary entry, store entries that span attributes
    \end{itemize}

\section{Dictionary Encoding and Decoding}
\begin{itemize}
    \item A dictionary needs to support two operations
    \begin{itemize}
        \item \textbf{Encoding}: For a givin uncompressed value, convert it into its compressed form
        \item \textbf{Decode}: For a given compressed value, convert it back into its original form
        \item No magic hash function will do this for us
        \item Requires two data structures to support operations in both directions
    \end{itemize}
\end{itemize}

    \subsection*{Order Preserving Compression}
    \begin{itemize}
        \item The encoded values need to support sorting in the same order as original values
    \end{itemize}

\section{Dictionary Implementations}
    \subsection*{Hash Table}
    \begin{itemize}
        \item Fast and compact
        \item Unable to support range and prefix queries
    \end{itemize}
    \subsection*{B+ Tree}
    \begin{itemize}
        \item Slower than a hash table and takes more memory
        \item Can support range and prefix queries
        \item Shared-Leaves Trees ~\cite{p283-binnig}
    \end{itemize}


% ==================================================================
% BIBLIOGRAPHY
% ==================================================================
\newpage
\bibliographystyle{abbrvnat}
\bibliography{11-compression}

\end{document}
