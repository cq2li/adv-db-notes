\documentclass[11pt]{article}

\newcommand{\lecturenumber}{12}
\newcommand{\lecturename}{Logging Protocols}
\newcommand{\lecturedata}{2018-02-26}

\usepackage{../dbnotes}

\begin{document}

\maketitle
\thispagestyle{plain}

%% ==================================================================
%% DATABASE RECOVERY
%% ==================================================================
\section{Database Recovery}
Database recovery algorithms are techniques to ensure database \textbf{consistency}, transaction 
\textbf{atomicity}, and \textbf{durability} despite failures.
Recovery Algorithms have two parts:
\begin{enumerate}
    \item
    Actions during normal transaction processing to ensure that the DBMS can recover from a failure
    
    \item
    Actions after a failure to recover the database to a state that ensures atomicity, 
    consistency, and durability.
\end{enumerate}

%% ----------------------------------------------------
%% Logging Schemes
%% ----------------------------------------------------
\subsection{Logging Schemes}

\textbf{Approach \#1 -- Physical Logging:}
\begin{itemize}
    \item
    Record the changes made to a specific record in the database.
    
    \item
    Example: Store the original value and the after value for an attribute that is changed by 
    a query.
\end{itemize}

\textbf{Approach \#2 -- Logical Logging:}
\begin{itemize}
    \item
    Record the high-level operations executed by transactions.
    
    \item
    Example: The \sql{UPDATE}, \sql{DELETE} and \sql{INSERT} queries invoked by a transaction.
\end{itemize}

Logical logging writes less data in each log record than physical logging.
Difficult to implement recovery with logical logging if you have concurrent transactions.
Its hard to determine which parts of the database may have been modified by a query before 
crash and it takes longer to recover because you have to re-execute every transaction over again.

%% ==================================================================
%% DISK-ORIENTED LOGGING AND RECOVERY
%% ==================================================================
\section{Disk-Oriented Logging and Recovery}
The ``gold standard'' for physical logging and recovery in a  disk-oriented DBMS is 
\textbf{ARIES} (Algorithms for Recovery and Isolation Exploiting Semantics)~\cite{p94-mohan}.
ARIES was invented by IBM Research in the early 1990s for \dbSys{IBM DB2}.
It relies on STEAL and NO-FORCE buffer pool management policies.

%% ----------------------------------------------------
%% Main Ideas
%% ----------------------------------------------------
\subsection{Main Ideas}

\textbf{Write-Ahead Logging:}
\begin{itemize}
    \item
    Any change is recorded in log on stable storage before the database change is written 
    to disk.
    
    \item
    Each log record is assigned a unique identifier (LSN).
\end{itemize}

\textbf{Repeating History During Redo:}
\begin{itemize}
    \item
    On restart, retrace actions and restore database to exact state before crash.
\end{itemize}

\textbf{Logging Changes During Undo:}
\begin{itemize}
    \item
    Record undo actions to log to ensure action is not repeated in the event of repeated 
    failures.
\end{itemize}

%% ----------------------------------------------------
%% Recovery Phases
%% ----------------------------------------------------
\subsection{Recovery Phases}

\textbf{Phase 1: Analysis}
\begin{itemize}
    \item
    Read the WAL to identify dirty pages in the buffer pool and active transactions at the time of 
    the crash.
\end{itemize}

\textbf{Phase 2: Redo}
\begin{itemize}
    \item
    Repeat all actions starting from an appropriate point in the log.
    
    \item
    Log redo operations in case of crash during recovery.
\end{itemize}

\textbf{Phase 3: Undo}
\begin{itemize}
    \item
    Reverse the actions of transactions that did not commit before the crash.
\end{itemize}

%% ----------------------------------------------------
%% Log Sequence Numbers
%% ----------------------------------------------------
\subsection{Log Sequence Numbers}
Every log record has a globally unique \textbf{log sequence numbers} (LSN) that is used to 
determine the serial order of those records.
The DBMS keeps track of various LSNs in both volatile and non-volatile storage to 
determine the order of almost everything.

%% ----------------------------------------------------
%% Optimizations
%% ----------------------------------------------------
\subsection{Optimizations}
\textbf{Group Commit:}
\begin{itemize}
    \item
    Batch together log records from multiple transactions and flush them together with a single 
    \texttt{fsync}. Amortizes the cost of I/O over several transactions.

    \item
    Logs are flushed either after a timeout or when the buffer gets full.
    
    \item
    Originally developed in \dbSys{IBM IMS Fastpath} in the 1980s.
\end{itemize}

\textbf{Early Lock Release:}
\begin{itemize}
    \item
    A transaction's locks can be released before its commit record is written to disk as long as 
    it does not return results to the client before becoming durable.
    
    \item
    Other transactions can read data updated by a \textbf{pre-committed} transaction become 
dependent on     it and also have to wait for their predecessor's log records to reach disk.
\end{itemize}

%% ==================================================================
%% IN-MEMORY DATABASE RECOVERY
%% ==================================================================
\section{In-Memory Database Recovery}
Recovery in an in-memory system is slightly easier than in a disk-oriented system because the DBMS 
does not have to worry about tracking dirty pages in case of a crash during recovery. The DBMS also 
does not need to store undo records.

It is still stymied by the slow sync time of non-volatile storage. This is why early papers 
(1980s) on recovery for in-memory DBMSs assume that there is non-volatile memory~\cite{p104-lehman}.

%% ==================================================================
%% SILOR
%% ==================================================================
\section{SiloR -- Physical Logging}
SiloR is an extended version of the Silo DBMS that supports logging and 
checkpoints~\cite{zheng-osdi14}.
It uses the epoch based OCC that we discussed previously~\cite{tu-sosp2013}.
It achieves high performance by parallelizing all aspects of logging, checkpointing, and recovery.

%% ----------------------------------------------------
%% Logging Protocol
%% ----------------------------------------------------
\subsection{Logging Protocol}
\begin{itemize}
    \item
    The DBMS assumes that there is one storage device per CPU socket.
    Each socket has one logger thread dedicated to writing data to that device.
    
    \item
    Worker threads are grouped per CPU socket.
    As the worker executes a transaction, it creates new log records that contain the values that 
were     written to the database (i.e., REDO).
    
    \item
    Each logger thread maintains a pool of log buffers that are given to its worker threads
    When a worker's buffer is full, it gives it back to the logger thread to flush to disk and 
    attempts to acquire a new one.
    If there are no available buffers, then the worker thread stalls.
\end{itemize}

%% ----------------------------------------------------
%% Log Files
%% ----------------------------------------------------
\subsection{Log Files}
The logger threads write buffers out to files.
\begin{itemize}
    \item
    After 100 epochs, it creates a new file.
    
    \item
    The old file is renamed with a marker indicating the max epoch of records that it 
    contains.
\end{itemize}

Log Record Format:
\begin{itemize}
    \item
    Id of the transaction that modified the record (\texttt{TID}).
    
    \item
    A set of value log triples (Table, Key, Value).
    
    \item
    The value can be a list of attribute + value pairs.
\end{itemize}

%% ----------------------------------------------------
%% Persistent Epoch
%% ----------------------------------------------------
\subsection{Persistent Epoch}
A special logger thread keeps track of the current persistent epoch (\texttt{pepoch}).
Special log file that maintains the high epoch that is durable across all the loggers.
Transactions that executed in epoch \textbf{$e$} can only release their results when the 
\texttt{pepoch} is durable to non-volatile storage.

%% ----------------------------------------------------
%% Recovery Protocol
%% ----------------------------------------------------
\subsection{Recovery Protocol}
\textbf{Phase 1: Load Last Checkpoint}
\begin{itemize}
    \item
    Install the contents of the last checkpoint that was saved into the database.
    
    \item
    All indexes have to be rebuilt.
\end{itemize}

\textbf{Phase 2: Replay Log}
\begin{itemize}
    \item
    Process logs in reverse order to reconcile the latest version of each tuple.
    
    \item
    First Check the \texttt{pepoch} file to determine the most recent persistent epoch.
    Any log record from after the \textbf{pepoch} is ignored.
    
    \item Log files are then processed from newest to oldest:
    \begin{itemize}
        \item
        Value logging is able to be replayed in any order.
        
        \item
        For each log record, the thread checks to see whether the tuple already exists.
        If it does not, then it is created with the value.
        If it does, then the tuple's value is overwritten only if the log \texttt{TID} is newer 
        than the tuple's \texttt{TID}.
    \end{itemize}
\end{itemize}

%% ==================================================================
%% VOLTDB
%% ==================================================================
\section{\dbSys{VoltDB} -- Logical Logging}
VoltDB uses a variant of logical logging called \textit{command logging}~\cite{malviya-icde14}.
The DBMS only records the stored procedure invocation:
\begin{itemize}
    \item
    Store procedure name.
    
    \item
    Input parameters.
    
    \item
    Additional safety checks to make sure that the DBMS executes the correct version of the stored 
    procedure.
\end{itemize}
        
The key issue with command logging is that if the log contains multi-node transactions, then if one 
node goes down and there are no more replicas, then the entire DBMS has to restart.

Command logging also only works if the DBMS uses a \textit{deterministic} concurrency control 
scheme. For a given state of the database, the execution of a serial schedule will always put 
the database in the same new state if the order of transactions is defined before they start 
executing and the transaction logic is deterministic.

Executing a deterministic transaction on the multiple copies of the same database in the same order 
provides strongly consistent replicates. This means that DBMS does not need to use an atomic commit 
protocol (e.g., two-phase commit) to make sure that all of the nodes are in the same state.

%% ----------------------------------------------------
%% Logging Protocol
%% ----------------------------------------------------
\subsection{Logging Protocol}
\begin{itemize}
    \item
    The DBMS logs the transaction command \textbf{before} it starts executing once a transaction has 
    been assigned its serial order.
    
    \item The node with the transaction's base partition is responsible for writing the log record:
    \begin{itemize}
        \item
        Remote partitions do not log anything.
        
        \item
        Replica nodes have to log just like their master.
    \end{itemize}
\end{itemize}

%% ----------------------------------------------------
%% Recovery Protocol
%% ----------------------------------------------------
\subsection{Recovery Protocol}
\begin{itemize}
    \item
    The DBMS loads in the last complete checkpoint from disk.
    
    \item
    Nodes then re-execute all of the transactions in the log that arrived after the checkpoint 
    started (oldest to newest).
        
    \item
    The amount of time elapsed since the last checkpoint in the log (roughly) determines how 
    long recovery will take. Transactions that are aborted first still have to be executed unless 
    there are hints in the log.
\end{itemize}

% ==================================================================
% BIBLIOGRAPHY
% ==================================================================
\newpage
\bibliographystyle{abbrvnat}
\bibliography{12-logging}

\end{document}
