\documentclass[11pt]{article}

\newcommand{\lecturenumber}{14}
\newcommand{\lecturename}{Scheduling}
\newcommand{\lecturedata}{2019-02-28}

\usepackage{../dbnotes}

\begin{document}

\maketitle
\thispagestyle{plain}

%% ==================================================================
%% QUERY EXECUTION
%% ==================================================================
\section{Query Execution}
A query plan generated by the optimizer is comprised of operators. An operator instance is an 
invocation of an operator on some segment of data. A task is the execution of a sequence of one or 
more operator instances.

%% ----------------------------------------------------
%% Scheduling
%% ----------------------------------------------------
\subsection*{Scheduling} 
For each query plan, the DBMS has to decide where, when and how to use it in a multi-core, 
multi-threaded environment. The DBMS needs to know the number of tasks it should use to divide the 
query. We also need to figure out how many CPU cores the DBMS should use and which CPU cores to 
execute the tasks on. After a task is completed, the DBMS needs to know where to store the output. 
Instead of leaving the details of scheduling to the OS, the DBMS should handle scheduling on its own 
as much as possible. The DBMS has more understanding on the functionality of queries and the shape 
of the data, so it is in a better position of deciding specific scheduling policies.

%% ==================================================================
%% PROCESS MODELS
%% ==================================================================
\section{Process Models}
We first focus on how to design a multi-threaded, multi-worker processing system for a DBMS. A 
DBMS's process model defines how the system is architected to support concurrent requests from a 
multi-user application. A worker is the DBMS component that is responsible for executing tasks on 
behalf of the client and returning the results. There are three different types of process models: 
\textit{Process per DBMS Worker}, \textit{Process Pool} and \textit{Thread per DBMS Worker}.

%% ----------------------------------------------------
%% Process Per Worker
%% ----------------------------------------------------
\subsection*{Process Per Worker}
Each worker is a separate OS process. It has its own pid and address space that is managed by the 
OS. Therefore, the only way for the workers to communicate with each other is through shared-memory. 
A process crash in this model also does not take down the entire system.

When a client sends out a request, it is received by the dispatcher that is responsible for handing 
out the request to some worker. The worker communicates with the client to take care of all the 
client's future requests. It then runs all the requests on the DBMS and returns the results to the 
client.

Back in 1980--1990s, \texttt{pthread} does not exist yet and there is no standard threading 
package. Since most OSs support multi-processes in shared-memory, so this approach is used in older 
DBMSs. 

Examples: \dbSys{IBM DB2}, \dbSys{Postgres}, \dbSys{Oracle}

%% ----------------------------------------------------
%% Process Pool
%% ----------------------------------------------------
\subsection*{Process Pool}
Process Pool is a variant model of Process Per Worker and still relies on the OS scheduler and 
shared-memory. 

Instead of binding a particular worker to a client, the dispatcher can hand off the request to any 
worker available in the process pool. To process queries in parallel, workers in the process pool 
can communicate with each other to divide tasks using shared-memory. Since all the workers still 
have their own separated address space, this approach can cause slowdowns due to bad CPU cache 
locality.

Examples: \dbSys{IBM DB2}, \dbSys{Postgres} (since 2015)

%% ----------------------------------------------------
%% Thread Per Worker
%% ----------------------------------------------------
\subsection*{Thread Per Worker}
Use a single process with multiple worker threads. This model no longer relies on the OS scheduling; 
the DBMS has to manage its own scheduling. The DBMS uses a pool of worker threads to handle requests 
(it may or may not use a dispatcher thread). The threads can write to shared address space that is 
protected by latches. However, a thread crash (may) kill the entire system.

Using a multi-threaded architecture has several advantages. It is easier to engineer, since we do 
not have to worry about shared memory and \texttt{pthread} is readily available. All the threads run 
in the same address space, so there is less overhead per context switch.


The thread per worker model does not imply it yields intra-query parallelism. Even if the DBMS is 
multi-threaded, it may still be executing one thread at a time (e.g., \dbSys{MySQL}).

Examples: \dbSys{IBM DB2}, \dbSys{MSSQL}, \dbSys{MySQL}, \dbSys{Oracle} (since 2014)


%% ==================================================================
%% DATA PLACEMENT
%% ==================================================================
\section{Data Placement}
Regardless of what worker allocation or task assignment policy the DBMS uses, it is important that 
workers operate on local data. The DBMS should be aware of it's underlying hardware's memory layout: 
whether it is uniform or non-uniform memory access.

%% ----------------------------------------------------
%% Uniform Memory Access
%% ----------------------------------------------------
\subsection*{Uniform Memory Access}
Each CPU has its own local cache. In order to read data from the memory DIMMs, the requests would go 
through the bus. The bus retrieves the data and handles cache invalidations. The cost of getting 
data from any one DIMM is roughly the same. The hardware covers lower-level details and provides 
cache-coherent guarantees.

%% ----------------------------------------------------
%% Non-Uniform Memory Access
%% ----------------------------------------------------
\subsection*{Non-Uniform Memory Access (NUMA)}
A newer form of memory access is NUMA. Each socket has its own cache and set of DIMMs. The DIMMs are 
physically close to the CPU, so the CPU can read/write to local memory quickly. The cost of getting 
data from a DIMM is higher if it is in a farther physical location. When a CPU tries to access a 
memory location in a different CPU, it has to go through the inter-connect to that socket and cache 
the results. Therefore, for in-memory DBMS, it is important for the DBMS know where to run its tasks 
in order to operate on data in close physical proximity.

The different CPU vendors refer to this inter-connect channel by different marketing terms: 
\textit{Intel UltraPath Interconnect} (2017) and \textit{AMD Infinity Fabric} (2017).

%% ----------------------------------------------------
%% Data Placement
%% ----------------------------------------------------
\subsection*{Data Placement}
The database can partition memory for a database and assign each partition to a CPU. By controlling 
and tracking the location of partitions, it can schedule operators to execute on workers at the 
closest CPU core.

%% ----------------------------------------------------
%% Memory Allocation
%% ----------------------------------------------------
\subsection*{Memory Allocation}
When the DBMS calls \texttt{malloc}, if the allocator does not already have a chunk of memory that 
it can give out, it will request memory from the OS. The allocator will extend the process's data 
segment. However, this new virtual memory is not immediately backed by physical memory; the OS only 
allocates physical memory when there is a page fault. There are two possible ways for the OS to 
allocate physical memory in a NUMA system:

\begin{itemize}
    \item \textbf{Interleaving:}
    Distribute allocated memory uniformly across CPUs.
  
    \item \textbf{First-Touch:}
    Distributed allocated memory at the CPU of the thread that accessed 
    the memory location that caused the page fault. This approach is more favored as physical 
    proximity increases CPU access in NUMA systems.
\end{itemize}

%% ----------------------------------------------------
%% Partitioning vs Placement
%% ----------------------------------------------------
\subsection*{Partition vs. Placement}
A DBMS's \textit{partitioning scheme} splits the database into disjoint subsets based on some 
policy. Some common partitioning schemes are: Round-robin, Attribute Ranges, Hashing and 
Partial/Full Replication. 

A DBMS's \textit{placement scheme} tells the DBMS where to put the partitions. Common 
placement schemes include Round-robin and Interleave Across Cores.

%% ==================================================================
%% SCHEDULING
%% ==================================================================
\section{Scheduling}
The DBMS uses scheduling to decide how to create a set of tasks from a logical query plan.

%% ----------------------------------------------------
%% Static Scheduling
%% ----------------------------------------------------
\subsection*{Static Scheduling}
The DBMS decides how many threads to use to execute the query when it generates the plan. 
Nothing changes while the query executes. The easiest approach is to have the DBMS use the same 
number of tasks as the number of cores.

This approach assumes the data is uniformly-distributed; each socket is expected to have the 
same amount of work. However, in practice, there are often discrepancies between the amount of work 
executed on each socket.

%% ----------------------------------------------------
%% Morsel-Driven Scheduling
%% ----------------------------------------------------
\subsection*{HyPer: Morsel-Driven Scheduling}
The DBMS dynamically schedules tasks that operate over horizontal partitions called "morsels" that 
are distributed across cores~\cite{Leis2014}. This approach uses one worker per CPU 
core, pull-based task assignment and round-robin data placement across the sockets. It also supports 
parallel, NUMA-aware operator implementations.

In \dbSys{HyPer}, the DBMS uses pull-based task assignment instead of separated dispatcher 
threads. The threads perform cooperative scheduling for each query plan using a single task queue. 
Each worker tries to select tasks that will execute on morsels that are local to it. If there are 
no local tasks, then the worker just pulls the next task from the global work queue.

%% ----------------------------------------------------
%% SAP HANA: NUMA-Aware Scheduler
%% ----------------------------------------------------
\subsection*{SAP HANA: NUMA-Aware Scheduler}
Because there is only one worker per core, \dbSys{HyPer} has to use work stealing because otherwise 
threads could sit idle waiting for stragglers. A solution to this is to use a lock-free hash table 
to maintain the global work queues.

\dbSys{SAP HANA} uses pull-based scheduling with multiple worker threads that are organized into 
\textit{thread groups} (i.e., pools)~\cite{Psaroudakis2015}. This NUMA-Aware Scheduler 
can dynamically adjust thread pinning based on whether a task is CPU memory bound. Each CPU can have 
multiple groups, and each group has a soft and hard priority queue. The DBMS uses a separate 
``watchdog'' thread to check whether groups are saturated and can reassign tasks dynamically.

Each thread group has a soft and hard priority task queues. Threads are only allowed to steal tasks 
from other groups' soft queues. There are four different pools of thread per group:

\begin{itemize}
    \item \textbf{Working:}
     Actively executing a task.
     
     \item \textbf{Inactive:}
     Blocked inside of the kernel due to a latch.
 
     \item \textbf{Free:}
     Sleeps for a little, wake up to see whether there is a new task to execute.
 
     \item \textbf{Parked:}
     Like free but does not wake up on its own.
\end{itemize}

Using thread groups allows cores to execute other tasks instead of just only queries.

Researchers also found that work stealing was not as beneficial for systems with a 
larger number of sockets. The overhead of moving data around on a machine becomes problematic on a 
larger scale.

%% ==================================================================
%% FLOW CONTROL
%% ==================================================================
\section{Flow Control}
If requests arrive at the DBMS faster than it can execute them, then the system becomes 
overloaded. The OS cannot help the DBMS. If the DBMS is memory bound, an out-of-memory (OOM) error 
is returned. If the DBMS is CPU bound, the requests queue will keep stalling and stacking future 
requests until the DBMS runs out of memory. The easiest DBMS solution to this is to simply crash. We 
introduce two additional control methods that are often used together in a DBMS:

\textbf{Admission Control:}
Abort new requests when the system believes that it will not have enough resources to execute that 
request. This approach appears more often in newer DBMS systems.

\textbf{Throttling:}
Delay the responses to clients to increase the amount of time between requests. 
This approach assumes a synchronous submission scheme (i.e., the client waits for response before 
sending the next request).

% ==================================================================
% BIBLIOGRAPHY
% ==================================================================
\newpage
\bibliographystyle{abbrvnat}
\bibliography{14-scheduling}

\end{document}
